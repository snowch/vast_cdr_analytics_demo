# docker-stack.yml
version: '3.8'

services:
  # This is our intelligent, pattern-driven generator, now scalable.
  telco-generator:
    image: telco-generator:latest # Use a tagged image for stack deployments
    build:
      context: ./telco-generator
    deploy:
      replicas: 4 # Start with 4 replicas. Scale with 'docker service scale'.
    env_file: .env # Load environment variables from .env file
    environment:
      # These are passed from the .env file
      - KAFKA_BROKER_URL=${KAFKA_BROKER_URL}
      - VASTDB_ENDPOINT=${VASTDB_ENDPOINT}
      - VASTDB_ACCESS_KEY=${VASTDB_ACCESS_KEY}
      - VASTDB_SECRET_KEY=${VASTDB_SECRET_KEY}
      - VASTDB_BUCKET=${VASTDB_BUCKET}
      - VASTDB_SCHEMA=${VASTDB_SCHEMA}
      - TOTAL_CELL_TOWERS=${TOTAL_CELL_TOWERS}
      # This magical variable tells each replica its number (e.g., 1, 2, 3, 4)
      # We use this to partition the subscribers.
      - GENERATOR_PARTITION_ID={{.Task.Slot}}
      - TOTAL_PARTITIONS=4 # Must match the number of replicas

  # This service's role is now purely to ingest high-volume analytical data.
  vast-db-connector:
    image: vast-db-connector:latest
    build:
      context: ./vast-db-connector
    deploy:
      replicas: 2 # Scale this based on Kafka topic partitions
    env_file: .env # Load environment variables from .env file
    environment:
      # These are passed from the .env file
      - KAFKA_BROKER_URL=${KAFKA_BROKER_URL}
      - VASTDB_ENDPOINT=${VASTDB_ENDPOINT}
      - VASTDB_ACCESS_KEY=${VASTDB_ACCESS_KEY}
      - VASTDB_SECRET_KEY=${VASTDB_SECRET_KEY}
      - VASTDB_BUCKET=${VASTDB_BUCKET}
      - VASTDB_SCHEMA=${VASTDB_SCHEMA}
      - KAFKA_TOPICS=cdrs,network_logs,customer_service
